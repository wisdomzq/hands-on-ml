{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "# 便于在jupyter中显示图像，inline表示将图像嵌入到jupyter中\n",
    "import torch\n",
    "import random\n",
    "from d2l import torch as d2l"
   ],
   "id": "2d064b04cd1283cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 生成数据集，构造一个带有噪声的线性数据集",
   "id": "1324e7043adfdc6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def synthetic_data(w, b, num_examples):  # @save\n",
    "    \"\"\"生成 y = Xw + b + 噪声。\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w))) # 生成一个服从正态分布(0,1)的数据集\n",
    "    y = torch.matmul(X, w) + b # 矩阵乘法\n",
    "    y += torch.normal(0, 0.01, y.shape) # 添加噪声\n",
    "    return X, y.reshape((-1, 1)) # 返回X和y,y的形状为(num_examples,1)"
   ],
   "id": "63492a5d73620b1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 通过函数构造数据集\n",
    "true_w = torch.tensor([2, -3.4]) #与X相乘的权重\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "# d2l内置了synthetic_data函数，可以直接调用"
   ],
   "id": "37ee80083b060fdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 绘制数据集的散点图",
   "id": "87b3350ea2ba22d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);\n",
    "# detach()返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置"
   ],
   "id": "1a9c7819a71fbbe6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 读取数据集\n",
   "id": "ec2caf27cf85a374"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples)) # 生成一个从0到num_examples-1的列表\n",
    "    random.shuffle(indices) # 用于将一个列表中的元素打乱顺序，不会生成新的列表，只是将原列表的次序打乱\n",
    "    # 随机读取小批量\n",
    "    for i in range(0, num_examples, batch_size): # 从0到num_examples，每次增加batch_size\n",
    "        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])\n",
    "        # 生成一个从i到min(i+batch_size,num_examples)的列表\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "        # yield关键字用于生成器，返回一个生成器对象"
   ],
   "id": "fa9fb5b1d73fdb68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  模型函数定义",
   "id": "9339f12a765a9ec9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 初始化模型参数\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 定义模型\n",
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型。\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失。\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 # 返回一个张量，张量的形状与y_hat相同\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降。\"\"\"\n",
    "    with torch.no_grad(): # 禁用梯度计算\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()"
   ],
   "id": "c2670b1b6ffc2481"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练函数",
   "id": "1a94c33f862f1cbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr=0.03\n",
    "num_epochs = 3\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(10, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # l是有关小批量X和y的损失\n",
    "        l.sum().backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, 10)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_l = loss(net(features, w, b), labels)\n",
    "            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')"
   ],
   "id": "3fd5a5ccf4dd4d2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ],
   "id": "1c5b748e568090d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
